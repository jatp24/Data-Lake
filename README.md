# ETL SPARKIFY <br>
Sparkify's data and user populationg has increased and with the growth a need for a data lake has become necessary. Their data is currently in a S3 bucket as json files that is seperated by user activity in log files and a directory with all the songs listed on their app. <br>

The goal of this project is to build a ELT pipeline that extract's Sparkify's data from the S3 bucket and the processes it using AWS ElasticMapReduce/Apache Spark, tranform the data and loads the data back into an S3 bucket as parquet files. <br>

## Quick Start <br>
* First, create an IAM role with write/read permission to S3 and fill in AWS access key (KEY) and secret (SECRET) in the dl.cfg file.
* Create an S3 bucket and replace the output_file variable with the S3 bucket location
* Create an EMR cluster with the following criteria:
    *Release emr-5.28.1
    *Spark: Spark 2.4.4 on Hadoop 2.8.5 YARN with Ganglia 3.7.2 and Zeppelin 0.8.2
    *Instance Type: (4 instances) m3.xlarge
* Run the etl.py file on a notebook hosted on the cluster or by connecting to the cluster via a SSH and using the python etl.py command.

<br>

## Data Resouces (S3 Buckets) <br>
Data resides in two directories that contain files in JSON format:

* s3a://udacity-dend/song_data : Contains metadata about a song and the artist of that song;
* s3a://udacity-dend/log_data : Consists of log files generated by the streaming app based on the songs in the dataset above;

<br>


## Tables <br>
    *Fact Table: songplays: attributes referencing to the dimension tables.
    *Dimension Tables: users, songs, artists and time tables.





## Build Status <br>

 The project has been completed and ready for deployment. <br>

 ## Code Style <br>

 Standard. <br>
